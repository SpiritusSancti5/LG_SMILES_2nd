{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"},"colab":{"name":"4.B.3.Inference.ipynb의 사본","provenance":[{"file_id":"1_SgjZbb2zRIyB6aU-6GqI8kzVpdk3ph_","timestamp":1602549588195}],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"JsjtlrfnlpRH","executionInfo":{"status":"ok","timestamp":1602514265453,"user_tz":-540,"elapsed":1284,"user":{"displayName":"lww bspl_2","photoUrl":"","userId":"17085094956192153852"}},"outputId":"6a0b5322-2687-44b7-d123-779dce3f9d5d","colab":{"base_uri":"https://localhost:8080/","height":369}},"source":["!nvidia-smi"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mon Oct 12 14:51:04 2020       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 455.23.05    Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   35C    P0    24W / 300W |      0MiB / 16130MiB |      0%      Default |\n","|                               |                      |                 ERR! |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"C3Mj8vkV8NvT"},"source":["# !fusermount -u google_drive"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8oiUYftf8Lxw"},"source":["from IPython.display import clear_output\n","\n","!add-apt-repository -y ppa:alessandro-strada/ppa; \n","!apt-get update;\n","!apt-get install -y google-drive-ocamlfuse;\n","clear_output()\n","\n","!mkdir google_drive;\n","!google-drive-ocamlfuse -headless -label dacon_smiles -id 406775554485-vqr231cgnpofc9mkm7sr0e3uq32emf11.apps.googleusercontent.com -secret iFy1t7pKRjOzBuWHbUB-cM8V;\n","\n","!sed -i 's/team_drive_id=0AOLSYhuNgxEsUk9PVA/team_drive_id=/' ~/.gdfuse/dacon_smiles/config\n","!sed -i 's/team_drive_id=/team_drive_id=0AOLSYhuNgxEsUk9PVA/' ~/.gdfuse/dacon_smiles/config\n","!google-drive-ocamlfuse -label dacon_smiles google_drive/\n","\n","# !fusermount -u google_drive\n","\n","# rdkit 2020.03.3 버전 다운로드\n","!pip install kora -q\n","import kora.install.rdkit\n","\n","import os\n","import os.path as pth\n","\n","### 저는 코랩에서 구글 드라이브를 네트워크 마운트해서 사용했기 때문에 경로가 이와 같이 됩니다.\n","google_drive_base_path = 'google_drive/chemical/'\n","\n","clear_output()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hvR2djPGDeLd"},"source":["import tensorflow as tf\n","from tensorflow.keras.preprocessing import image\n","import cv2\n","\n","import matplotlib.pyplot as plt\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.utils import shuffle\n","\n","import numpy as np\n","import pandas as pd\n","import os\n","import os.path as pth\n","import time\n","from tqdm import tqdm\n","\n","from rdkit import Chem\n","from rdkit import DataStructs\n","from rdkit import RDLogger\n","RDLogger.DisableLog('rdApp.*')  \n","\n","from IPython.display import clear_output\n","\n","from multiprocessing import Process, Queue\n","import datetime\n","from IPython.display import clear_output\n","\n","import gc"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QaQVGupzDeLk"},"source":["import tensorflow.keras as keras\n","import tensorflow.keras.layers as layers\n","from tensorflow.keras.layers import Input\n","from tensorflow.keras.models import Model, load_model\n","from tensorflow.keras.layers import Conv2D, Dense, MaxPooling2D, AveragePooling2D, GlobalAveragePooling2D\n","from tensorflow.keras.layers import Activation, BatchNormalization\n","from tensorflow.keras.layers import Concatenate\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.callbacks import Callback\n","from tensorflow.keras.optimizers import SGD\n","\n","import numpy as np\n","import tensorflow.keras.backend as K"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7ExHSf3GDeLp"},"source":["os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n","os.environ['HDF5_USE_FILE_LOCKING'] = 'FALSE'\n","\n","# gpus = tf.config.experimental.list_physical_devices('GPU')\n","# tf.config.experimental.set_memory_growth(gpus[0], True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wJZyn-xUER_o"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CwXYuR2VUMsu"},"source":["데이터를 구글 드라이브에서 wget을 통해 가져옵니다.  \n","이 방식을 많이 사용하면 구글 드라이브 다운로드 제한에 걸리지만, 마운트하는 것에 비해 빠르기에 사용하였습니다."]},{"cell_type":"code","metadata":{"id":"A7InxulwESbq"},"source":["import requests\n","\n","from multiprocessing import Pool\n","from functools import partial\n","\n","import zipfile\n","from google.colab import drive\n","\n","\n","def download_file_from_google_drive(id_, destination):\n","    URL = \"https://docs.google.com/uc?export=download\"\n","\n","    session = requests.Session()\n","    response = session.get(URL, params = { 'id' : id_ }, stream = True)\n","    token = get_confirm_token(response)\n","    if token:\n","        params = { 'id' : id_, 'confirm' : token }\n","        response = session.get(URL, params = params, stream = True)\n","        \n","    basename = response.headers['Content-Disposition'].split(';')[1].split('filename=')[1].replace('\\\"', '')\n","    # if 'Content-Disposition' not in response.headers:\n","    #     basename = id_\n","    # else:\n","        # basename = response.headers['Content-Disposition'].split(';')[1].split('filename=')[1].replace('\\\"', '')\n","    full_dst_filenname = pth.join(destination, basename)\n","    save_response_content(response, full_dst_filenname)\n","    return full_dst_filenname\n","\n","def get_confirm_token(response):\n","    for key, value in response.cookies.items():\n","        if key.startswith('download_warning'):\n","            return value\n","\n","    return None\n","\n","def save_response_content(response, destination):\n","    CHUNK_SIZE = 32768\n","    with open(destination, \"wb\") as f:\n","        for chunk in response.iter_content(CHUNK_SIZE):\n","            if chunk: # filter out keep-alive new chunks\n","                f.write(chunk)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XUHOxtyBERQI"},"source":["file_id_list = [\n","    # '1nHqhEySQdWus7tiU5Bnd2vwfqHAiGxGo', # train.zip\n","    # '1Rw_HeEE6y57oosmWQsGPgk2T4jrykxm-', # train_half.zip\n","    # '1E6D8S8Sa5VcUi2MCOt_J5EQT41pS9C-6', # train_quat.zip\n","    # '1tEUNK9N5FlRt0tSNpyWIpZn5VSMPUCFh', # test.zip\n","    '1JvRvYMZQGCYXGcankPKiyzNvkCib0MXo', # train.csv\n","    '1JGruGKR-Tv1aKgONY0r_LW4zuwkLEyDX', # sample_submission.csv\n","]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FDjlcxWuEn2M","executionInfo":{"status":"ok","timestamp":1602514344397,"user_tz":-540,"elapsed":65763,"user":{"displayName":"lww bspl_2","photoUrl":"","userId":"17085094956192153852"}},"outputId":"f444eaf9-9480-4e33-d848-d2be4fb38f82","colab":{"base_uri":"https://localhost:8080/","height":70}},"source":["destination = 'data' ### YOUR_DOWNLOAD_PATH\n","if not pth.exists(destination):\n","    os.makedirs(destination, exist_ok=True)\n","\n","    filename_list = []\n","\n","    # ### Use single process\n","    # for file_id in file_id_list:\n","    #     filename = download_file_from_google_drive(id_=file_id, destination=destination)\n","    #     print('{} is done!'.format(filename))\n","    #     filename_list.append(filename)\n","\n","    ### If you want to download more faster\n","    download_func = partial(download_file_from_google_drive, destination=destination)\n","    with Pool(4) as pool:\n","        for i, filename in tqdm(enumerate(pool.imap_unordered(download_func, file_id_list)), total=len(file_id_list)):\n","            print('{} is done!'.format(filename))\n","            filename_list.append(filename)"],"execution_count":null,"outputs":[{"output_type":"stream","text":[" 50%|█████     | 1/2 [00:01<00:01,  1.06s/it]"],"name":"stderr"},{"output_type":"stream","text":["data/sample_submission.csv is done!\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 2/2 [00:03<00:00,  1.51s/it]"],"name":"stderr"},{"output_type":"stream","text":["data/train.csv is done!\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"gmUtzx0_DeLu"},"source":["### ChemHub에서 수집한 데이터를 이용해 학습\n","\n","제공 된 데이터는 ChemHub에서 순서대로 수집한 SMILES로 유사 구조들이 밀집되어 있을 가능성이 높습니다."]},{"cell_type":"code","metadata":{"id":"YDwBQMw3DeLv"},"source":["data_base_path = 'data'\n","os.makedirs(data_base_path, exist_ok=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CTcI1t_YDeL5"},"source":["# test_dataset_name = 'test.tfrecords'\n","test_dataset_name = 'test.zip'\n","\n","train_csv_name = 'train.csv'\n","sample_submission_name = 'sample_submission.csv'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_r9pstrtd3pn"},"source":["if not pth.exists(pth.join(data_base_path, test_dataset_name)):\n","    os.system('cp {}/data/{} {}'.format(google_drive_base_path, test_dataset_name, data_base_path))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b77Fg_tNgQYT","executionInfo":{"status":"ok","timestamp":1602518702208,"user_tz":-540,"elapsed":5680,"user":{"displayName":"lww bspl_2","photoUrl":"","userId":"17085094956192153852"}},"outputId":"66b86226-ce63-4a9e-f551-2dbe409e0240","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["os.system('unzip {} -d {}'.format(pth.join(data_base_path, test_dataset_name), data_base_path))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{"tags":[]},"execution_count":62}]},{"cell_type":"code","metadata":{"id":"RpTpam53DeMD"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cEFxPdMADeMd"},"source":["encoder_base_channel = 8\n","encoder_model_base = 'CustomDenseNet-121'\n","encoder_model_name = 'Autoencoder_{}_trts_basech_{:03d}'.format(encoder_model_base, encoder_base_channel)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3_om8q94DeMh"},"source":["encoder_model_base_path = pth.join(data_base_path, 'checkpoint')\n","encoder_model_path = pth.join(encoder_model_base_path, encoder_model_name)\n","encoder_model_gdrive_path = pth.join(google_drive_base_path, 'model', 'checkpoint', encoder_model_name)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d8h7l-Wv72wI"},"source":["os.makedirs(encoder_model_path, exist_ok=True)\n","target_checkpoint_filename = sorted(os.listdir(encoder_model_gdrive_path))[-1]\n","encoder_model_filename = pth.join(encoder_model_path, target_checkpoint_filename)\n","encoder_model_gdrive_filename = pth.join(encoder_model_gdrive_path, target_checkpoint_filename)\n","\n","if not pth.exists(encoder_model_filename):\n","    os.system('cp {} {}'.format(encoder_model_gdrive_filename, encoder_model_filename))\n","    while os.path.getsize(encoder_model_gdrive_filename) != os.path.getsize(encoder_model_filename):\n","        os.system('cp {} {}'.format(encoder_model_gdrive_filename, encoder_model_filename))  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ronG3pq2DeMl"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rRo9Li3_DeMt"},"source":["n_mht = 512\n","# n_layer = 4\n","n_dff = 1024\n","n_head = 8\n","# dropout = 0.1\n","dropout = 0\n","decoder_model_name = 'trfrm_mht_{}_layer_{}_dff_{}_head_{}_DO_{}'.format(\n","    n_mht, n_layer, n_dff, n_head, dropout\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z7t-gyN_DeMw"},"source":["# model_name = 'enc-tr_{}_dec-tr_{}_len-100-all'.format(encoder_model_name, decoder_model_name)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tHyOPO9EU0OE"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q1Sne2msU0lx"},"source":["최대 길이 및 워드 임베딩을 설정합니다"]},{"cell_type":"code","metadata":{"id":"blb3y2VoDeMz"},"source":["def calc_max_length(tensor):\n","    return max(len(t) for t in tensor)\n","\n","# max_length = calc_max_length(train_captions)\n","max_length = 100"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cWbfSFfcDeM2"},"source":["tokenizer = tf.keras.preprocessing.text.Tokenizer(lower=False, char_level=True)\n","all_token_list = [\n","    'c', 'C', '(', ')', '1', 'O', '=', '2', 'N', '<', '>', 'n', '[',\n","    ']', '3', '@', 'H', 'l', 'S', '-', 'F', '+', '4', 's', 'o', '#',\n","    'B', 'r', '.', '/', 'P', 'i', 'I', '5', '\\\\', 'e', 'A', 'a', 'g',\n","    '6', 'u', 't', 'T', 'M', 'b', 'K', 'Z', '8', 'd', '9', 'R', 'G',\n","    '7', 'L', 'V', 'h', 'W', 'p', 'm', 'E', 'Y', '0', 'U', 'f', 'D',\n","    'y', 'k', 'X', ' ', '^', '%', '$'\n","]\n","tokenizer.fit_on_texts(all_token_list)\n","top_k = len(tokenizer.word_index)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QRjwQRDRDeM9"},"source":["### 하이퍼 파라미터 및 학습에 필요한 변수 지정"]},{"cell_type":"code","metadata":{"id":"zjCn_eZwDeM-"},"source":["BATCH_SIZE = 100\n","BUFFER_SIZE = 100\n","d_model = n_mht\n","num_layers = n_layer\n","dff = n_dff\n","num_heads = n_head\n","vocab_size = top_k # + 1\n","dropout_rate = dropout"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wdjjfYUvDeNA"},"source":["is_sgd = True\n","\n","EPOCHS = 200\n","learning_rate = 1e-4"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Pl224IkzDeNJ"},"source":["### 모델 정의"]},{"cell_type":"code","metadata":{"id":"WpSqXzEqDeNb"},"source":["def get_angles(pos, i, d_model):\n","    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n","    return pos * angle_rates\n","\n","\n","def positional_encoding(position, d_model):\n","    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n","                          np.arange(d_model)[np.newaxis, :],\n","                          d_model)\n","    # apply sin to even indices in the array; 2i\n","    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n","    # apply cos to odd indices in the array; 2i+1\n","    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n","    pos_encoding = angle_rads[np.newaxis, ...]\n","\n","    return tf.cast(pos_encoding, dtype=tf.float32)\n","\n","\n","def create_padding_mask(seq):\n","    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n","\n","    # add extra dimensions to add the padding\n","    # to the attention logits.\n","    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n","\n","\n","def create_look_ahead_mask(size):\n","    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n","    return mask  # (seq_len, seq_len)\n","\n","\n","def scaled_dot_product_attention(q, k, v, mask):\n","    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n","\n","    # scale matmul_qk\n","    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n","    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n","\n","    # add the mask to the scaled tensor.\n","    if mask is not None:\n","        scaled_attention_logits += (mask * -1e9)  \n","\n","    # softmax is normalized on the last axis (seq_len_k) so that the scores\n","    # add up to 1.\n","    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n","\n","    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n","\n","    return output, attention_weights    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vxPQyILqDeNh"},"source":["class MultiHeadAttention(tf.keras.layers.Layer):\n","    def __init__(self, d_model, num_heads):\n","        super(MultiHeadAttention, self).__init__()\n","        self.num_heads = num_heads\n","        self.d_model = d_model\n","\n","        assert d_model % self.num_heads == 0\n","\n","        self.depth = d_model // self.num_heads\n","\n","        self.wq = tf.keras.layers.Dense(d_model)\n","        self.wk = tf.keras.layers.Dense(d_model)\n","        self.wv = tf.keras.layers.Dense(d_model)\n","\n","        self.dense = tf.keras.layers.Dense(d_model)\n","\n","    def split_heads(self, x, batch_size):\n","        \"\"\"Split the last dimension into (num_heads, depth).\n","        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n","        \"\"\"\n","        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n","        return tf.transpose(x, perm=[0, 2, 1, 3])\n","\n","    def call(self, v, k, q, mask):\n","        batch_size = tf.shape(q)[0]\n","\n","        q = self.wq(q)  # (batch_size, seq_len, d_model)\n","        k = self.wk(k)  # (batch_size, seq_len, d_model)\n","        v = self.wv(v)  # (batch_size, seq_len, d_model)\n","\n","        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n","        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n","        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n","\n","        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n","        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n","        scaled_attention, attention_weights = scaled_dot_product_attention(\n","            q, k, v, mask)\n","\n","        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n","\n","        concat_attention = tf.reshape(scaled_attention, \n","                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n","\n","        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n","\n","        return output, attention_weights\n","\n","\n","def point_wise_feed_forward_network(d_model, dff):\n","    return tf.keras.Sequential([\n","      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n","      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n","    ])\n","\n","\n","class DecoderLayer(tf.keras.layers.Layer):\n","    def __init__(self, d_model, num_heads, dff, rate=0.1):\n","        super(DecoderLayer, self).__init__()\n","\n","        self.mha1 = MultiHeadAttention(d_model, num_heads)\n","        self.mha2 = MultiHeadAttention(d_model, num_heads)\n","\n","        self.ffn = point_wise_feed_forward_network(d_model, dff)\n","\n","        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","\n","        self.dropout1 = tf.keras.layers.Dropout(rate)\n","        self.dropout2 = tf.keras.layers.Dropout(rate)\n","        self.dropout3 = tf.keras.layers.Dropout(rate)\n","\n","\n","    def call(self, x, enc_output, training, \n","           look_ahead_mask, padding_mask):\n","        # enc_output.shape == (batch_size, input_seq_len, d_model)\n","\n","        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n","        attn1 = self.dropout1(attn1, training=training)\n","        out1 = self.layernorm1(attn1 + x)\n","\n","        attn2, attn_weights_block2 = self.mha2(\n","            enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n","        attn2 = self.dropout2(attn2, training=training)\n","        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n","\n","        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n","        ffn_output = self.dropout3(ffn_output, training=training)\n","        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n","\n","        return out3, attn_weights_block1, attn_weights_block2 \n","\n","\n","class Decoder(tf.keras.layers.Layer):\n","    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n","               maximum_position_encoding, rate=0.1):\n","        super(Decoder, self).__init__()\n","\n","        self.d_model = d_model\n","        self.num_layers = num_layers\n","\n","        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n","        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n","\n","        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n","                           for _ in range(num_layers)]\n","        self.dropout = tf.keras.layers.Dropout(rate)\n","\n","    def call(self, x, enc_output, training, \n","           look_ahead_mask, padding_mask):\n","        seq_len = tf.shape(x)[1]\n","        attention_weights = {}\n","\n","        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n","        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n","        x += self.pos_encoding[:, :seq_len, :]\n","\n","        x = self.dropout(x, training=training)\n","\n","        for i in range(self.num_layers):\n","            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n","                                                 look_ahead_mask, padding_mask)\n","\n","            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n","            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n","\n","        # x.shape == (batch_size, target_seq_len, d_model)\n","        return x, attention_weights\n","\n","\n","class CNN_Encoder(tf.keras.Model):\n","    def __init__(self, embedding_dim):\n","        super(CNN_Encoder, self).__init__()\n","        target_checkpoint_filename = sorted(os.listdir(encoder_model_path))[-1]\n","        image_autoencoder = load_model(pth.join(encoder_model_path, target_checkpoint_filename))\n","        image_features_extract_model = image_autoencoder.get_layer(encoder_model_base)\n","#         image_features_extract_model.trainable = False\n","        self.feature_extract_model = image_features_extract_model\n","        self.fc = tf.keras.layers.Dense(embedding_dim, activation='relu')\n","        \n","    def call(self, x):\n","        x = self.feature_extract_model(x)\n","        x = tf.keras.layers.Reshape((-1, x.shape[3]))(x)\n","        x = self.fc(x)\n","        return x\n","\n","\n","class ImageCaptioningTransformer(tf.keras.Model):\n","    def __init__(self, num_layers, d_model, num_heads, dff,\n","               target_vocab_size, pe_target, rate=0.1):\n","        super(ImageCaptioningTransformer, self).__init__()\n","\n","        self.encoder = CNN_Encoder(d_model)\n","\n","        self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n","                               target_vocab_size, pe_target, rate)\n","\n","        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n","\n","    def call(self, inp, tar, training, look_ahead_mask, dec_padding_mask):\n","        enc_output = self.encoder(inp)\n","\n","        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n","        dec_output, attention_weights = self.decoder(\n","            tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n","\n","        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n","\n","        return final_output, attention_weights\n","\n","\n","def create_masks(tar):\n","    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n","    dec_target_padding_mask = create_padding_mask(tar)\n","    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n","\n","    return combined_mask        "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZlWPQg01DeNl"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OLx9Vr8YDeNz"},"source":["### Optimizer 및 사용되는 함수"]},{"cell_type":"code","metadata":{"id":"embwHpO9DeNz"},"source":["class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n","    def __init__(self, d_model, warmup_steps=5):\n","        super(CustomSchedule, self).__init__()\n","\n","        self.d_model = d_model\n","        self.d_model = tf.cast(self.d_model, tf.float32)\n","\n","        self.warmup_steps = warmup_steps\n","\n","    def __call__(self, step):\n","        arg1 = tf.math.rsqrt(step)\n","        arg2 = step * (self.warmup_steps ** -1.5)\n","\n","        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xG3C7r2JDeN5"},"source":["if is_sgd == True:\n","    optimizer = tf.keras.optimizers.SGD(lr=learning_rate)\n","else:\n","    optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n","\n","# learning_rate = CustomSchedule(d_model)\n","# optimizer = tf.keras.optimizers.Adam(\n","#     learning_rate, beta_1=0.9, beta_2=0.98, \n","#     epsilon=1e-9\n","# )\n","\n","loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n","    from_logits=True, reduction='none'\n",")\n","\n","def loss_function(real, pred):\n","    mask = tf.math.logical_not(tf.math.equal(real, 0))\n","    loss_ = loss_object(real, pred)\n","\n","    mask = tf.cast(mask, dtype=loss_.dtype)\n","    loss_ *= mask\n","\n","    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y_a92R58DeOI"},"source":["def calculate_similarity(real, pred):\n","#     pred = np.array(list(map(np.array, pred)))\n","#     pred = np.moveaxis(pred, (0,1,2), (1,0,2))\n","    pred = np.argmax(pred, axis=-1)\n","#     print(real[:5], pred[:5])\n","    real = real.numpy()\n","    \n","    score_list = []\n","    for score_i, (each_pred, each_real) in enumerate(zip(pred, real)): \n","        each_pred = ''.join([tokenizer.index_word.get(mol_i, '') for mol_i in each_pred])\n","        each_pred = each_pred.split('>')[0]\n","        m_pred = Chem.MolFromSmiles(each_pred)\n","        if m_pred == None:\n","            score_list.append(0)\n","            continue\n","        each_real = ''.join([tokenizer.index_word.get(mol_i, '') for mol_i in each_real])\n","        each_real = each_real[1:-1]\n","        m_real = Chem.MolFromSmiles(each_real)\n","        \n","        fp_pred = Chem.RDKFingerprint(m_pred)\n","        fp_real = Chem.RDKFingerprint(m_real)\n","        target_similarity = DataStructs.FingerprintSimilarity(fp_real,fp_pred)\n","        score_list.append(target_similarity)\n","        \n","    return score_list"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1m6nzpENDeOo"},"source":["# 인퍼런스 부분"]},{"cell_type":"markdown","metadata":{"id":"zHNwj2pcDeOp"},"source":["### Predict 함수 정의"]},{"cell_type":"code","metadata":{"id":"Zbc_HhEZDeOp"},"source":["# @tf.function\n","def predict(img_tensor):\n","    decoder_input = tf.expand_dims([tokenizer.word_index['<']] * img_tensor.shape[0], 1)\n","    output = decoder_input\n","    \n","    real_output = np.zeros((img_tensor.shape[0], max_length+1))\n","    real_output[...] = tokenizer.word_index[' ']\n","    real_output[:,0] = tokenizer.word_index['<']\n","    \n","    is_end_array = np.zeros((img_tensor.shape[0])).astype(np.bool)\n","\n","    for i in range(max_length):\n","        if not all(is_end_array):\n","            combined_mask = create_masks(output)\n","            predictions, attention_weights = captioning_transformer(\n","                inp=img_tensor, tar=output, training=False, \n","                look_ahead_mask=combined_mask, dec_padding_mask=None\n","            )\n","        else:\n","            break\n","        predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n","        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n","        real_output[:,i+1] = predicted_id.numpy().squeeze()\n","        output = tf.concat([output, predicted_id], axis=-1)\n","        is_end_array = (is_end_array | (real_output[:,i+1]==tokenizer.word_index['>']))\n","        # print(i, is_end_array)\n","\n","    real_output[:,-1] = tokenizer.word_index['>']\n","#     return tf.squeeze(output, axis=0), attention_weights\n","    return real_output, attention_weights"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wm_XpNNCFktp"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HN_i6jMpDePB"},"source":["# TestSet 예측"]},{"cell_type":"markdown","metadata":{"id":"un0tzGCgfvlF"},"source":["Transformer는 RNN과 다르게 initial random state가 없기 때문에 같은 모델에서 반복해서 예측해도 다른 결과를 얻을 수 없습니다.  \n","따라서 예측한 결과가 기본적인 smiles 분자식에 맞지 않을 경우엔, 다른 체크포인트에서 예측한 결과로 메꾸거나, 다른 모델에서 예측한 결과를 통해 항상 유효한 smiles식을 생성할 수 있도록 합니다.\n"]},{"cell_type":"code","metadata":{"id":"C_eBtuvQDePG","executionInfo":{"status":"ok","timestamp":1602517637253,"user_tz":-540,"elapsed":846,"user":{"displayName":"lww bspl_2","photoUrl":"","userId":"17085094956192153852"}},"outputId":"01c0fbb2-3683-4eb1-a3a6-85ff96acc8e8","colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["TEST_PATH = pth.join(data_base_path, 'test')\n","\n","with open(pth.join(data_base_path, 'sample_submission.csv'), 'r') as csv_file:\n","    data = csv_file.read()\n","    \n","test_img_path = []\n","\n","for line in data.split('\\n')[1:-1]:\n","    image_id, _ = line.split(',')\n","    full_image_path = pth.join(TEST_PATH, image_id)\n","\n","    test_img_path.append(full_image_path)\n","  \n","test_tfrecord_list = np.array([img_name+'.tfrecords' for img_name in test_img_path])\n","test_tfrecord_list[:3]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['data/test/test_0.png.tfrecords', 'data/test/test_1.png.tfrecords',\n","       'data/test/test_2.png.tfrecords'], dtype='<U34')"]},"metadata":{"tags":[]},"execution_count":51}]},{"cell_type":"code","metadata":{"id":"62wC8AZgDePI"},"source":["image_feature_description_test = {\n","    'image_raw': tf.io.FixedLenFeature([], tf.string),\n","    # 'filename': tf.io.FixedLenFeature([], tf.string),\n","}\n","\n","def _parse_image_function_test(example_proto):\n","    return tf.io.parse_single_example(example_proto, image_feature_description_test)\n","\n","def map_func_pred(target_record):\n","    img = target_record['image_raw']\n","    img = tf.image.decode_jpeg(img, channels=3)\n","    img = tf.dtypes.cast(img, tf.float32)\n","    return img\n","\n","def prep_func_pred(image):\n","    result_image = image\n","    return result_image"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"or6fA9OkDePO"},"source":["### 결과 예측"]},{"cell_type":"code","metadata":{"id":"gkNkt7covBXL"},"source":["model_1_name = 'enc-tr_Autoencoder_CustomDenseNet-121_trts_basech_008_dec-tr_trfrm_mht_512_layer_4_dff_1024_head_8_DO_0_len-100-all'\n","model_2_name = 'enc-tr_Autoencoder_CustomDenseNet-121_trts_basech_008_dec-tr_trfrm_mht_512_layer_8_dff_1024_head_8_DO_0_len-100-all'\n","\n","checkpoint_base_path = pth.join(google_drive_base_path, 'model', 'checkpoint')\n","checkpoint_path_list = [\n","    pth.join(model_1_name, 'ckpt-155'),\n","    pth.join(model_2_name, 'ckpt-120'),\n","    pth.join(model_1_name, 'ckpt-141'),\n","    pth.join(model_1_name, 'ckpt-147'),\n","    pth.join(model_2_name, 'ckpt-111'),\n","    pth.join(model_1_name, 'ckpt-149'),\n","    pth.join(model_2_name, 'ckpt-121'),\n","    pth.join(model_1_name, 'ckpt-151'),\n","    pth.join(model_1_name, 'ckpt-144'),\n","    pth.join(model_2_name, 'ckpt-109'),\n","    pth.join(model_1_name, 'ckpt-139'),\n","    pth.join(model_1_name, 'ckpt-138'),\n","    pth.join(model_1_name, 'ckpt-137'),\n","]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y5SW8c8r5KkQ"},"source":["구글 드라이브에서 바로 체크포인트를 불러오면 I/O 명령이 너무 많은 탓인지 종종 disk quota limit 에러가 뜹니다.  \n","그래서 저는 코랩 로컬에 옮겨와서 모델을 불러오지만, 실행하실 때는 첨부한 체크포인트를 data/checkpoint 이하 경로에 놓고 실행해주시면 됩니다.\n"]},{"cell_type":"code","metadata":{"id":"PyT_ibmKba_5","executionInfo":{"status":"ok","timestamp":1602531209599,"user_tz":-540,"elapsed":40968,"user":{"displayName":"lww bspl_2","photoUrl":"","userId":"17085094956192153852"}},"outputId":"066c9eeb-149e-4b4a-fe9d-29001989011f","colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["# ### 해당 코드는 구글 드라이브에서 코랩 로컬로 필요한 체크포인트를 옮기는 코드입니다.\n","# for target_checkpoint_path in tqdm(checkpoint_path_list):\n","#     model_name = pth.dirname(target_checkpoint_path)\n","#     temp_checkpoint_path = pth.join('data', 'checkpoint', model_name)\n","#     os.makedirs(temp_checkpoint_path, exist_ok=True)\n","\n","#     checkpoint_basename = pth.basename(target_checkpoint_path)\n","#     temp_checkpoint_filename = pth.join(temp_checkpoint_path, checkpoint_basename)\n","#     origin_checkpoint_filename = pth.join(checkpoint_base_path, target_checkpoint_path)\n","\n","#     if not pth.exists(temp_checkpoint_filename+'.data-00000-of-00001'):\n","#         os.system('cp {}* {}'.format(origin_checkpoint_filename, temp_checkpoint_path))\n","#         while os.path.getsize(temp_checkpoint_filename+'.data-00000-of-00001') != os.path.getsize(origin_checkpoint_filename+'.data-00000-of-00001'):\n","#             os.system('cp {}* {}'.format(origin_checkpoint_filename, temp_checkpoint_path))  "],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","\n","  0%|          | 0/13 [00:00<?, ?it/s]\u001b[A\u001b[A\n","\n"," 77%|███████▋  | 10/13 [00:16<00:04,  1.62s/it]\u001b[A\u001b[A\n","\n"," 85%|████████▍ | 11/13 [00:25<00:07,  3.78s/it]\u001b[A\u001b[A\n","\n"," 92%|█████████▏| 12/13 [00:31<00:04,  4.61s/it]\u001b[A\u001b[A\n","\n","100%|██████████| 13/13 [00:40<00:00,  3.09s/it]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"BbcqF5eAINkt","executionInfo":{"status":"ok","timestamp":1602537569811,"user_tz":-540,"elapsed":2224442,"user":{"displayName":"lww bspl_2","photoUrl":"","userId":"17085094956192153852"}},"outputId":"07819237-7f25-416d-f71a-201290417549","colab":{"base_uri":"https://localhost:8080/","height":700}},"source":["result_submission_path = pth.join('submission')\n","os.makedirs(result_submission_path, exist_ok=True)\n","\n","submission_df = pd.read_csv(pth.join(data_base_path, 'sample_submission.csv'))\n","\n","pred_summary_array = np.zeros((len(submission_df), max_length)).astype(np.int32)\n","pred_summary_array[...] = tokenizer.word_index[' ']\n","unavaliable_mask = np.ones(len(submission_df)).astype(np.bool)\n","\n","for target_checkpoint_path in checkpoint_path_list:\n","    model_name = pth.dirname(target_checkpoint_path)\n","    temp_checkpoint_path = pth.join('data', 'checkpoint', model_name)\n","    os.makedirs(temp_checkpoint_path, exist_ok=True)\n","\n","    checkpoint_basename = pth.basename(target_checkpoint_path)\n","    temp_checkpoint_filename = pth.join(temp_checkpoint_path, checkpoint_basename)\n","    origin_checkpoint_filename = pth.join(checkpoint_base_path, target_checkpoint_path)\n","\n","    ### GPU 메모리 관리를 위해 모델이 이미 존재하면 지워줍니다.\n","    if 'captioning_transformer' in locals():\n","        K.clear_session()\n","        del(captioning_transformer)\n","        gc.collect()\n","\n","    target_model_layer = int(model_name.split('_layer_')[1].split('_dff_')[0])\n","    captioning_transformer = ImageCaptioningTransformer(\n","        target_model_layer, d_model, num_heads, dff,\n","        vocab_size, pe_target=max_length,\n","        rate=dropout_rate\n","    )\n","\n","    ckpt = tf.train.Checkpoint(\n","        captioning_transformer=captioning_transformer, \n","        optimizer=optimizer\n","    )\n","    ckpt_manager = tf.train.CheckpointManager(ckpt, temp_checkpoint_filename, max_to_keep=25)\n","    ckpt.restore(temp_checkpoint_filename)\n","\n","    dataset_test = tf.data.TFRecordDataset(test_tfrecord_list[unavaliable_mask], compression_type='GZIP')\n","    dataset_test = dataset_test.map(_parse_image_function_test, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","    # dataset_test = dataset_test.cache()\n","    dataset_test = dataset_test.map(map_func_pred, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","    dataset_test = dataset_test.batch(BATCH_SIZE)\n","    dataset_test = dataset_test.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n","\n","    test_num_steps = int(np.ceil(unavaliable_mask.sum()/BATCH_SIZE))\n","    # test_num_steps = len(test_img_path)\n","    test_result = []\n","    for img_tensor in tqdm(dataset_test, total=test_num_steps, position=0, leave=True):\n","        pred_result, attention_weights = predict(img_tensor)\n","        test_result.append(pred_result)\n","    test_result = np.concatenate(test_result)\n","\n","    preds = []\n","    for rid in range(len(test_result)):\n","        pred = ''.join([tokenizer.index_word[i] for i in test_result[rid]])\n","        pred = pred[1:].split('>')[0]\n","        preds.append(pred)\n","\n","    mask_backup = unavaliable_mask.copy()\n","    temp_array = pred_summary_array[mask_backup].copy()\n","    temp_mask = np.ones(unavaliable_mask.sum()).astype(np.bool)\n","    error_idx = []\n","    for i, pred in enumerate(preds):\n","        m = Chem.MolFromSmiles(pred)\n","        if m == None:\n","            error_idx.append(i)\n","        else:\n","            temp_array[i] = test_result[i][:max_length]\n","            temp_mask[i] = False\n","    pred_summary_array[mask_backup] = temp_array\n","    unavaliable_mask[unavaliable_mask] = temp_mask\n","\n","    error_idx = np.array(error_idx)\n","    error_idx_ = error_idx.copy()\n","    print('Model name:', temp_checkpoint_filename)\n","    print('Error count:', len(error_idx))\n","\n","preds = []\n","for each_pred in pred_summary_array.astype(np.int32):\n","    pred = ''.join([tokenizer.index_word.get(i, '') for i in each_pred])\n","    pred = pred[1:].split('>')[0]\n","    preds.append(pred)\n","preds = np.array(preds)\n","preds[unavaliable_mask] = 'CCC(C)Oc1cc(N)nc(S(=O)(=O)c2cccc(C)c2)n1' ### 끝까지 예측하지 못한 이미지는 임의의 값을 집어넣습니다.\n","submission_df['SMILES'] = preds\n","\n","### submission file 저장\n","result_submission_path = 'submission'\n","os.makedirs(result_submission_path, exist_ok=True)\n","\n","today_str = datetime.date.today().strftime('%Y%m%d')\n","result_filename = 'model_1_2_ensemble.csv'\n","submission_csv_fileaname = pth.join(result_submission_path, '_'.join([today_str, result_filename]))\n","submission_df.to_csv(submission_csv_fileaname, index=False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 200/200 [31:50<00:00,  9.55s/it]\n"],"name":"stderr"},{"output_type":"stream","text":["Model name: data/checkpoint/enc-tr_Autoencoder_CustomDenseNet-121_trts_basech_008_dec-tr_trfrm_mht_512_layer_4_dff_1024_head_8_DO_0_len-100-all/ckpt-155\n","Error count: 46\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 1/1 [00:14<00:00, 14.55s/it]\n"],"name":"stderr"},{"output_type":"stream","text":["Model name: data/checkpoint/enc-tr_Autoencoder_CustomDenseNet-121_trts_basech_008_dec-tr_trfrm_mht_512_layer_8_dff_1024_head_8_DO_0_len-100-all/ckpt-120\n","Error count: 9\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 1/1 [00:09<00:00,  9.28s/it]\n"],"name":"stderr"},{"output_type":"stream","text":["Model name: data/checkpoint/enc-tr_Autoencoder_CustomDenseNet-121_trts_basech_008_dec-tr_trfrm_mht_512_layer_4_dff_1024_head_8_DO_0_len-100-all/ckpt-141\n","Error count: 4\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 1/1 [00:08<00:00,  8.33s/it]\n"],"name":"stderr"},{"output_type":"stream","text":["Model name: data/checkpoint/enc-tr_Autoencoder_CustomDenseNet-121_trts_basech_008_dec-tr_trfrm_mht_512_layer_4_dff_1024_head_8_DO_0_len-100-all/ckpt-147\n","Error count: 2\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 1/1 [00:14<00:00, 14.59s/it]\n"],"name":"stderr"},{"output_type":"stream","text":["Model name: data/checkpoint/enc-tr_Autoencoder_CustomDenseNet-121_trts_basech_008_dec-tr_trfrm_mht_512_layer_8_dff_1024_head_8_DO_0_len-100-all/ckpt-111\n","Error count: 2\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 1/1 [00:07<00:00,  7.65s/it]\n"],"name":"stderr"},{"output_type":"stream","text":["Model name: data/checkpoint/enc-tr_Autoencoder_CustomDenseNet-121_trts_basech_008_dec-tr_trfrm_mht_512_layer_4_dff_1024_head_8_DO_0_len-100-all/ckpt-149\n","Error count: 2\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 1/1 [00:11<00:00, 11.65s/it]\n"],"name":"stderr"},{"output_type":"stream","text":["Model name: data/checkpoint/enc-tr_Autoencoder_CustomDenseNet-121_trts_basech_008_dec-tr_trfrm_mht_512_layer_8_dff_1024_head_8_DO_0_len-100-all/ckpt-121\n","Error count: 2\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 1/1 [00:08<00:00,  8.45s/it]\n"],"name":"stderr"},{"output_type":"stream","text":["Model name: data/checkpoint/enc-tr_Autoencoder_CustomDenseNet-121_trts_basech_008_dec-tr_trfrm_mht_512_layer_4_dff_1024_head_8_DO_0_len-100-all/ckpt-151\n","Error count: 1\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 1/1 [00:07<00:00,  7.81s/it]\n"],"name":"stderr"},{"output_type":"stream","text":["Model name: data/checkpoint/enc-tr_Autoencoder_CustomDenseNet-121_trts_basech_008_dec-tr_trfrm_mht_512_layer_4_dff_1024_head_8_DO_0_len-100-all/ckpt-144\n","Error count: 1\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 1/1 [00:14<00:00, 14.96s/it]\n"],"name":"stderr"},{"output_type":"stream","text":["Model name: data/checkpoint/enc-tr_Autoencoder_CustomDenseNet-121_trts_basech_008_dec-tr_trfrm_mht_512_layer_8_dff_1024_head_8_DO_0_len-100-all/ckpt-109\n","Error count: 1\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 1/1 [00:09<00:00,  9.35s/it]\n"],"name":"stderr"},{"output_type":"stream","text":["Model name: data/checkpoint/enc-tr_Autoencoder_CustomDenseNet-121_trts_basech_008_dec-tr_trfrm_mht_512_layer_4_dff_1024_head_8_DO_0_len-100-all/ckpt-139\n","Error count: 1\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 1/1 [00:08<00:00,  8.66s/it]\n"],"name":"stderr"},{"output_type":"stream","text":["Model name: data/checkpoint/enc-tr_Autoencoder_CustomDenseNet-121_trts_basech_008_dec-tr_trfrm_mht_512_layer_4_dff_1024_head_8_DO_0_len-100-all/ckpt-138\n","Error count: 1\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 1/1 [00:08<00:00,  8.05s/it]\n"],"name":"stderr"},{"output_type":"stream","text":["Model name: data/checkpoint/enc-tr_Autoencoder_CustomDenseNet-121_trts_basech_008_dec-tr_trfrm_mht_512_layer_4_dff_1024_head_8_DO_0_len-100-all/ckpt-137\n","Error count: 1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"srG8ArGaenO9"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dHMNuv9CvLhS","executionInfo":{"status":"ok","timestamp":1602538512938,"user_tz":-540,"elapsed":913,"user":{"displayName":"lww bspl_2","photoUrl":"","userId":"17085094956192153852"}},"outputId":"385c3fa1-3a9c-4ef9-9f99-c74e3675c569","colab":{"base_uri":"https://localhost:8080/","height":187}},"source":["!head submission/20201012_model_1_2_ensemble.csv"],"execution_count":null,"outputs":[{"output_type":"stream","text":["file_name,SMILES\n","test_0.png,COc1cc(N2CCOCC2)ccc1C=C1C(=O)NC(=O)N(C2CCCCC2)C1=O\n","test_1.png,CC(=NN=C1C(=O)Nc2cc(Cl)ccc21)c1cc2ccccc2oc1=O\n","test_2.png,CC1CN(Cc2ccccc2NC(=O)NCC(C)(C)C(N)=O)CC(C)O1\n","test_3.png,Cc1cccc2c(NCCc3nnc4ccccn34)c(C#N)cnc12\n","test_4.png,COc1cccc(-c2cc(CNC(=O)C(c3ccc(F)cc3)N(C)C)no2)c1\n","test_5.png,Cc1c(Cl)cccc1NC(=O)C1CCN(S(C)(=O)=O)c2cc(Cl)ccc2O1\n","test_6.png,Cc1ccc(C(=O)N2CCc3ccccc32)cc1N\n","test_7.png,CCC(Nc1cc(C(=O)N2CCCC2)ccn1)c1ccc(C)cn1\n","test_8.png,Cn1c(=NC(=O)c2ccccc2O)[nH]c2cc(Cl)ccc21\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2v-87UJLvPOQ"},"source":[""],"execution_count":null,"outputs":[]}]}